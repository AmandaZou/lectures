---
title: 'STAT656: Cross-validation and parallelism'
subtitle: We will investigate cross-validation using the caret package and introduce parallel processing.
output:
  html_document: default
---

# Reading in the data
```{r}
require(caret)
require(dplyr)
```

# General framework
The data splitting/resampling implementations in caret return the indicies instead of the split data.  So, the workflow is usually to:

* Split the indices into the subsets/folds/bootstrap samples
* Grab the observations that have that index


We will just look at the classic data set "Iris"

```{r}
str(iris)
```
# Data splitting

We can do "data splitting" using [createDataPartition](http://topepo.github.io/caret/data-splitting.html).


If we want to make a training/test split, the process is pretty straightforward.  We pass the supervisor as the first argument so that the splitting can be done in a stratified manner.

The 'p' argument states the size of the subsets, given as a fraction.  The default output is as a "list" object.  I prefer to output  use list = FALSE, which creates a matrix data structure with the indices of the observations in the subset along the rows.  I then convert this to a vector with "as. vector"

```{r dataSplitting}
set.seed(1)
n          = nrow(iris)
trainIndex = createDataPartition(iris$Species, p = .5, list = FALSE) %>% as.vector(.)
testIndex  = (1:n)[-trainIndex]

table(iris$Species[trainIndex])
table(iris$Species[testIndex])

role            = rep('train',n)
role[testIndex] = 'test'
```

We can make a quick plot with two of the features to look at the randomization into the two groups.  
```{r dataSplittingPlot}
ggplot(data = cbind(iris,role)) + geom_point( aes(x = Sepal.Length, y = Petal.Width, color = role) )
```

Adding more subsets, such as a validation set, takes a little more work.  Note that the "times" argument does a new randomization into two subsets, it doesn't make more than 2 subsets.

```{r dataSplittingValidation}
trainIndex = createDataPartition(iris$Species, p = .5, list = FALSE) %>% as.vector(.)

validSplit = createDataPartition(iris$Species[-trainIndex], p = .5, list = FALSE) %>% as.vector(.)

n          = nrow(iris)
testIndex  = (1:n)[-trainIndex][-validSplit]
validIndex = (1:n)[-trainIndex][validSplit]

role             = rep('train',n)
role[testIndex]  = 'test'
role[validIndex] = 'validation'
```

We can make a quick plot with two of the features to look at the randomization into the three groups.  

```{r dataSplittingValidationPlot}
ggplot(data = cbind(iris,role)) + geom_point( aes(x = Sepal.Length, y = Petal.Width, color = role) )
```


# Resampling

### K-fold CV
For K-fold cross-validation (CV), we can use a related function: "createFolds".  It creates a list object with K entries.  There are two main use cases for createFolds:

* if we use "returnTrain = FALSE", then each of the K entries correspond to the indices of the Kth fold. 
* if we use "returnTrain = TRUE", then each of the K entries correspond to the training indices; that is, the K-1 folds all combined together, leaving out the Kth fold.

Usually, I will returnTrain = FALSE as this uses less space

```{r KfoldCVsplits}
set.seed(1)
K = 10
cvSplitsFolds = createFolds(iris$Species, k = K, returnTrain = FALSE)

cvSplitsFolds$Fold01
```

A simple example would be estimating a mean using the (K-1) folds and then getting the squared error on the Kth fold.  We will look at the feature 'Sepal.width'

```{r KfoldCV}
CVresultsVec = rep(0,K)
for(k in 1:K){
  muHat_k         = mean(iris$Sepal.Width[ -cvSplitsFolds[[k]] ])
  CVresultsVec[k] = mean((muHat_k - iris$Sepal.Width[cvSplitsFolds[[k]]])**2)
}
CVperformanceEstimate = mean(CVresultsVec)
CVperformanceEstimate
```
Let's compare this to the "apparent error rate" or the "training error"

```{r apparentErrorExample}
muHat         = mean(iris$Sepal.Width)
apparentError = mean( (muHat - iris$Sepal.Width) **2 )

apparentError
```
Note that for very simple procedures, the difference between the apparent error and the CV estimate is usual small.  But if we USE the apparent error to set the model flexibility, then we will always end up choosing the most flexible/complex choice.


### Bootstrap

We can use the caret function 'createResample' in oder to facilitate the bootstrap
```{r bootstrap}
set.seed(1)
nBootstrapDraws = 100
bootstrapDraws  = createResample(iris$Species, times  = nBootstrapDraws)
bootstrapDraws[[1]]
```

Notice two things:

* not every observation (index in this case) appears in each (bootstrap) sample
* some observations appear multiple times

On average, we should see about 2/3 unique observations in each bootstrap sample:

```{r bootstrapSizes}
mean(sapply(bootstrapDraws, function(x){ length(unique(x))/length(x)}))
```

The idea is that we could use the remaining 1/3 (or so) observations, which are called "out of bag (OOB)" to estimate performance and the 2/3 that are in bag to estimate the model parameters

A simple example would be estimating a mean using the in bag and then getting the squared error on the OOB observations

```{r bootstrapErrorEstimateExample}
bootstrapResultsVec = rep(0,nBootstrapDraws)
for(b in 1:nBootstrapDraws){
  
  muHat_b                = mean(iris$Sepal.Width[ bootstrapDraws[[b]] ])
  bootstrapResultsVec[b] = mean( (muHat_b - iris$Sepal.Width[ -unique(bootstrapDraws[[b]]) ])**2 )
}
bootstrapPerformanceEstimate = mean(bootstrapResultsVec)
bootstrapPerformanceEstimate
```

Let's compare the apparent error, CV error, the bootstrap error, and the "0.632 bootstrap":

* The apparent error: `r apparentError`
* The CV error: `r CVperformanceEstimate`
* The bootsrap error: `r bootstrapPerformanceEstimate`
* The 0.632 bootstrap error: `r bootstrapPerformanceEstimate*0.632 + (1-0.632)*apparentError`


As we mentioned in class, there we will mainly use CV to judge model performance.  But, we will use the concepts behind the bootstrap and OOB again later.

# Model fitting

Continuing with KNN, we can fit KNN to some training data and get the test set predictions

### Some KNN details

KNN doesn't technically have any model parameters. The one free parameter, the number of neighbors (K), is really a tuning parameter as it governs model flexibility.  The "fitting" of KNN is really computing the distances between all the observations.  This requires a definition of distance (Euclidean) and that all the features are float point numbers.

### The two interfaces to R

R has two ways of parameterizing functions for model fitting:

* Via a formula
* Specifying the feature/supervisor via x = X, y = Y.

Here is a simulated example

```{r interfaceExample}
n = 100
set.seed(1)
loan = data.frame('defaultLoan' = ifelse(rbinom(n,1,.1) == 1, 'default','noDefault'),
                  'experian'    = rnorm(n,mean=750,sd=10),
                  'equifax'     = rnorm(n,mean=750,sd=10),
                  'transUnion'  = rnorm(n,mean=750,sd=10),
                  'income'      = rnorm(n,mean=40000,sd=3000),
                  'defaultPast' = ifelse(rbinom(n,1,.1) == 1, 'default','noDefault'))
```
#### Formula

The formula interface is convenient for 

* Making the code more readable
* Taking care of qualitative features automatically

For instance, the syntax looks more like:

'defaultLoan ~ experian + equifax + transUnion + income + defaultPast, data = loan'

All the objects listed in the modeling statement exist as objects inside the data.frame 'loan'.

Here, defaultPast is qualitative.  Most models will require 'defaultPast' be converted into a dummy variable (see Chapter 3.6).  R recognizes this as a common task and hence does it automatically, which can be convenient.

```{r formulaExample}
knn3(defaultLoan ~ experian + equifax + transUnion + income + defaultPast, data = loan)
# or, if we want to specify every feature as being included:

knn3(defaultLoan ~ ., data = loan)
```

Question: There is a major problem with the whole idea of applying distances to these observations.  What is it?

#### x = X, y = Y

The problem with the formula interface is that as it relies on interpretting the symbolic expression into a mathematical one, it can be very slow.  The alternative specification is to manually create the feature matrix X via converting all qualitative features into dummy variables

The caret package has some nice functionality for doing this.  Let's look at the possible values for defaultPast, In this case, we saved defaultPast as a character data structure.  We need to convert it into a factor, which is what R is expecting a qualitative feature to be encoded as

```{r xXyYexampleFactor}
loan = loan %>%  mutate(defaultPast = factor(defaultPast))
levels(loan %>% select(defaultPast) %>% unlist(.))
```

The caret implementation still uses the formula interface, but it is faster due to the two-step procedure
```{r xXyYexampleFactorCaret}
factorConversion = dummyVars(~., data = select(loan,-defaultLoan), fullRank = TRUE)
featureMatrix    = predict(factorConversion, loan,drop2nd = TRUE)
head(featureMatrix)
```

```{r xXyYexample}
knn3(x = featureMatrix, y = select(loan,defaultLoan) %>% unlist, k = 3)
```


# Parallelism Introduction

The main topic in parallelism we will be covering is 'embarrassingly' or 'trivially' parallel scenarios.  Despite the names, it can be very useful.

Suppose we have a cluster of processors/cores.  There are many ways in R to do parallelism that depends on

* Your operating system (in particular, whether it supports forking)
* The type of server

We will use the [doParallel(https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) package.

Note that for simple commands, we can use 'system.time', which is similar to proc.time.
```{r systemTime}
system.time(Sys.sleep(10))
```

We can detect the number of cores or system has:
```{r numberOfCores}
require(doParallel)
nCores = detectCores()
nCores
```
This system has `r nCores` cores.

Let's try using 2 cores.
To do parallel on a general system, we can set up the following back end, which allocates 2 cores to be available for parallel processing.

One way to implement parallelism is to take for loops and convert them to foreach.  Note that the syntax for the 'for loop' changed slightly.
```{r parallelBackend2}
cl = makeCluster(2)
registerDoParallel(cl)

system.time({ foreach(i=1:3) %dopar% Sys.sleep(10) })
```


It's always a good idea to clean up the session by stopping that cluster of nodes/processors/nodes.
```{r parallelCleanUp2}
stopCluster(cl)
```

Let's try this again, but with 3 cores
```{r parallelBackend3}
cl = makeCluster(3)
registerDoParallel(cl)

system.time({ foreach(i=1:3) %dopar% Sys.sleep(10) })
stopCluster(cl)
```


```{r KfoldCVparallel}
cl = makeCluster(nCores)
registerDoParallel(cl)

set.seed(1)
K = 6
cvSplitsFolds = createFolds(iris$Species, k = K, returnTrain = FALSE)


CVresultsVec = rep(0,K)
srt = proc.time()[3]
foreach(k = 1:K) %dopar% {
  muHat_k         = mean(iris$Sepal.Width[ -cvSplitsFolds[[k]] ])
  CVresultsVec[k] = mean((muHat_k - iris$Sepal.Width[cvSplitsFolds[[k]]])**2)
}
end = proc.time()[3]
totalTimePar = end - srt

stopCluster(cl)

CVresultsVec = rep(0,K)
srt = proc.time()[3]
for(k in 1:K){
  muHat_k         = mean(iris$Sepal.Width[ -cvSplitsFolds[[k]] ])
  CVresultsVec[k] = mean((muHat_k - iris$Sepal.Width[cvSplitsFolds[[k]]])**2)
}
end = proc.time()[3]
totalTimeSeq = end - srt
```
The total parallel time is `r totalTimePar` and sequential time is `r totalTimeSeq`.  Note that in this small of a problem, the costs of parallelism outweigh the benefits